---
title: "Coursera Pratical Machine Learning"
author: "Icaro C. Bombonato"
date: "June 2015"
output: html_document
---

```{r, eval = FALSE, echo = FALSE}
# Code Analysis:
library(doParallel)
library(caret)

#Read data
data <- read.csv("pml-training.csv", header = TRUE, na.strings = c("", "NA"), stringsAsFactors = FALSE)

#Take a look ate the data
View(data)
summary(data)

# All NA in Column?
SumNA <- sapply(data, function(x)sum(is.na(x)))
#Drop all columns where NA is larger then 90% off the data
colstoDrop <-names(SumNA[SumNA>19622 * .90]);   
data <- data[,!names(data) %in% colstoDrop]

# Verifying complete cases
sum(complete.cases(data))
sum(!complete.cases(data))

# Looking and transforming the data
str(data)
data$classe <- as.factor(data$classe)

#Drop columns related to the subject
colstoDrop2 <-c("X","user_name",
                "raw_timestamp_part_1",
                "raw_timestamp_part_2",
                "cvtd_timestamp", "new_window", "num_window"); 
data <- data[,!names(data) %in% colstoDrop2]

set.seed(23456)

# create training set indexes with 10% of data
inTrain <- createDataPartition(y=data$classe,p=0.1, list=FALSE)
# subset data to training
train <- data[inTrain,]
# subset data to test
test <- data[-inTrain,]

#Remove high correlated data. > 70% correlation
descrCor <-  cor(train[,-53])
highlyCorDescr <- findCorrelation(descrCor, cutoff = .70)
filteredDescr <- train[,-highlyCorDescr]
descrCor2 <- cor(subset(filteredDescr, select =-c(classe)))

#Removing unused variables from train and original data
data <- data[,names(data) %in% c(colnames(descrCor2), "classe")]
train <- train[,names(train) %in% c(colnames(descrCor2), "classe")]

#Looking for NearZeroVariance data
nzv <- nearZeroVar(train, saveMetrics= TRUE)
nzv

#Enable parallel
cl <- makeCluster(detectCores())
registerDoParallel(cl)
# Feature Selection using cross validation
control <- rfeControl(functions=rfFuncs, method="cv", number=5)
# Run the RFE algorithm
classeColumnIndex <- ncol(train)
rfeResults <- rfe(train[,-classeColumnIndex], train[,c("classe")], 
                  sizes=c(1:(classeColumnIndex-1)), 
                  rfeControl=control)
                  
# Close parallel connections
stopCluster(cl)

# plot the results
plot(rfeResults, type=c("g", "o"))

# Looking at the top 10 variables
head(varImp(rfeResults), 10)

# create training set indexes with 30% of data.
# You can take more than that, but due to time of training, I pick only 30%.
# My first try with 75% took more than 8 hours...
inTrain <- createDataPartition(y=data$classe,p=0.30, list=FALSE)
# subset data to training
train <- data[inTrain,]
# subset data to test
test <- data[-inTrain,]

#Enable parallel
cl <- makeCluster(detectCores())
registerDoParallel(cl)

# Make model with 10 best variables
modelFit.Best <- train(classe ~ magnet_dumbbell_z    +
                           roll_dumbbell        +
                           pitch_forearm        +
                           magnet_belt_z        +
                           roll_forearm         +
                           gyros_belt_z         +
                           magnet_arm_x         +
                           gyros_dumbbell_y     +
                           total_accel_belt     +
                           accel_forearm_x, data = train, 
                     method="rf", prox = TRUE, 
                     allowParallel = TRUE)

# Close parallel connections
stopCluster(cl)

# Looking the results, error rate, accuracy, etc...
modelFit.Best
modelFit.Best$finalModel

# Predict in new/unseen data. (test data)
predictions <- predict(modelFit.Best, newdata = test)

# Results in test data
confusionMatrix(predictions, test$classe)
```


### Synopsis
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

In this project/analysis, our goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants and predict the manner in which they did the exercise.

### Cleaning the data

After load the data and take a look on it, I noted that are 100 variables missing in ~97% of the data. Since 3% is a very small set of data to make predictions, I remove these variables.

Since we are not interest in which subject did the exercise, I removed all variables that are related to the subject.

After that, I take a look at the correlation of the 52 variables left and cut off all with more then 0.7 absolute value of correlation. I did this with the **findCorrelation** function of the caret package. So I ended with 33 variables and started to build the model around them.

### Model Building

I used the [caret package](http://topepo.github.io/caret/) for building the model, training the data, predict new cases and estimate the accuracy of the model.

For this classification problem, I choose to use the Random Forest algorithm.

I used the **createDataPartition()** function to split the data in two separated samples and used one for training the data and the other to validate the results. That way, we can see how the model performs in new data and detect any problem before put it on production.

I started with a very small subset of the data to train the model faster, 10%, and pick the best one, after that, I trained the best model using 30% of the data to get a better accuracy.

I used the **rfeControl** and **rfe** function, [Recursive Feature Elimination](http://topepo.github.io/caret/featureselection.html), to train the first model and rank the variables, since complex models take a long time to run and sometimes 
the performance change are VERY small. And then, I plot the Accuracy x Variables to see the results and I choose to pick the 10 most important variables.

![FeatureSelection](featureSelection.png)

After taking a look at the graph, I choosed 10 variables to make the final model. The variables were selected using the **varImp(modelFit)** function, that list the variables by order of importance to the model.

And then I made my final model:

```{r, eval = FALSE}
modelFit.Best <- train(classe ~ magnet_dumbbell_z    +
                           roll_dumbbell        +
                           pitch_forearm        +
                           magnet_belt_z        +
                           roll_forearm         +
                           gyros_belt_z         +
                           magnet_arm_x         +
                           gyros_dumbbell_y     +
                           total_accel_belt     +
                           accel_forearm_x, data = train, 
                     method="rf", prox = TRUE, 
                     allowParallel = TRUE)
```

And the results for the model are bellow:

```{r, eval=FALSE}
Random Forest 

5889 samples
  33 predictor
   5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Bootstrapped (25 reps) 

Summary of sample sizes: 5889, 5889, 5889, 5889, 5889, 5889, ... 

Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
   2    0.9287708  0.9098260  0.005616632  0.007082752
   6    0.9208420  0.8998129  0.006531468  0.008228253
  10    0.9088271  0.8846037  0.007046107  0.008899502

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 2. 
```

I used the default **Bootstrap Resampling** technique to **Cross Validate** the data besides K–fold Cross Validation.

According to John Kuhn, creator of the caret package, this procedure, bootstrap resampling, has low variance but non–zero bias when compared to K–fold Cross Validation.

You can find more about it at this paper [Predictive Modeling with R and the caret Package](http://www.edii.uclm.es/~useR-2013/Tutorials/kuhn/user_caret_2up.pdf)

We can see that the model has an **Accuracy of ~93% and Kappa value of ~0.91** wich is considered pretty good.

Since the model uses resampling, we can take a look of how it **estimates out of sample error** in unseen data, through the Out Of Bag (OOB) estimate of error, in this case **6.25%**

```{r, eval = FALSE}
Call:
 randomForest(x = x, y = y, mtry = param$mtry, proximity = TRUE,      allowParallel = TRUE) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 6.25%
Confusion matrix:
     A    B   C   D    E class.error
A 1623   16  12  17    6  0.03046595
B   44 1027  53  10    6  0.09912281
C   22   49 940  14    2  0.08471276
D   22    5  46 885    7  0.08290155
E    4    5  13  15 1046  0.03416436
```

We can validate the model accuracy using the test data and see the results using the confusionMatrix function:

```{r, eval = FALSE}
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 3813  113   36   40   10
         B   42 2388   78   23   16
         C   25  105 2253  143   40
         D   15   44   20 2013   33
         E   11    7    8   32 2425

Overall Statistics
                                          
               Accuracy : 0.9388          
                 95% CI : (0.9346, 0.9427)
    No Information Rate : 0.2844          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9225          
 Mcnemar s Test P-Value : < 2.2e-16       

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9762   0.8988   0.9407   0.8943   0.9608
Specificity            0.9797   0.9856   0.9724   0.9902   0.9948
Pos Pred Value         0.9504   0.9376   0.8780   0.9473   0.9766
Neg Pred Value         0.9904   0.9760   0.9873   0.9795   0.9912
Prevalence             0.2844   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2777   0.1739   0.1641   0.1466   0.1766
Detection Prevalence   0.2921   0.1855   0.1868   0.1547   0.1808
Balanced Accuracy      0.9780   0.9422   0.9566   0.9423   0.9778
```

And we can see that the accuracy was even better with our test data, confirming the model statistics.

### Disclaimer

Since the assignment tells the sentence bellow and the work should not be reproducible, I omitted most of the code for cleaning the data and some part of my analysis,  but I tried to explain it the best that I could.

> **Reproducibility**  
> Due to security concerns with the exchange of R code, your code will not be run during the evaluation  by your classmates. Please be sure that if they download the repo, they will be able to view the compiled HTML version of your analysis.

You can access the compiled html at my github repo for this project:  
http://github.com/ibombonato/coursera-machine-learning-project

Or see it directly on the browser:  
http://ibombonato.github.io/coursera-machine-learning-project/
